from langchain_core.output_parsers import StrOutputParser
from langchain_core.messages import ChatMessage
from langchain_core.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
from langchain_core.prompts import load_prompt
from langchain.memory import ConversationSummaryBufferMemory
from langchain.schema.runnable import RunnablePassthrough
import streamlit as st
import os
# import openai

st.set_page_config(
    page_title="ì±—ë´‡",
    page_icon="ğŸ¤–",
    layout="wide",
    initial_sidebar_state="expanded"
)

# íƒ€ì´í‹€ ì ìš© ì˜ˆì‹œ
st.title('â¤ ìë…€ì™€ ëŒ€í™”í•˜ëŠ” ë¶€ëª¨ ì±—ë´‡ â¤')
st.subheader('ëŒ€í™”í•˜ê³  ì‹¶ì€ ìë…€ì™€ ë‚´ê°€ ëˆ„ê°€ë˜ê³  ì‹¶ì€ì§€ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”')

col1, col2, col3 = st.columns(3)
with col1 : 
    age = st.selectbox('ìë…€ì˜ ë‚˜ì´ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”',(10, 15, 20, 30, 40))
with col2 : 
    gender = st.selectbox('ìë…€ì˜ ì„±ë³„ì„ ì„ íƒí•´ì£¼ì„¸ìš”',('ë”¸', 'ì•„ë“¤'))
with col3 : 
    parents = st.selectbox('ë¶€ëª¨(ë‚˜)ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”',("ì—„ë§ˆ","ì•„ë¹ "))


# ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ ì„¤ì •
prompt = """
[ì •ì˜]
ë‚˜ëŠ” {age}ì„¸ {gender} ìë…€ ì±—ë´‡ì´ì•¼
{parents}ì™€ {concept}ë¥¼ ë‚˜ëˆ„ëŠ” ì±—ë´‡ì´ì•¼.
{age}ì„¸ {gender} ìë…€ì…ì¥ì—ì„œ ëŒ€í™”ë¥¼ ì´ì–´ê°€ì¤˜

[ì—­í• ]
AI : {age}ì„¸ {gender} ìë…€
ì§ˆë¬¸ì : {age}ì„¸ {gender}ì˜ {parents}

[ëŒ€í™” ì£¼ì œ]
ì§ˆë¬¸ìì˜ ì˜ë„ì— ë§ê²Œ ê·€ì—¬ìš´ ìë…€ì˜ ì¼ìƒì„ ë§í•´ì¤˜

ì˜ˆ)
ì§ˆë¬¸ì : {gender} ë­í•˜ë‹ˆ?
AI : {parents}, ë‚˜ ì§€ê¸ˆ ê³µë¶€í•˜ëŠ” ì¤‘ì´ì•¼ ã… ã…œ ì‹œí—˜ì´ ê³§ ì´ì–ì•„ ã… ã…œ
ì§ˆë¬¸ì : ë°¥ ì°¨ë¦´ê±´ë°, ë­ ë¨¹ê³  ì‹¶ì€ê±´ ì—†ì–´?
AI : ì•„ë¬´ ìƒê°ì—†ì–´ ê·¸ëƒ¥ ê°„ë‹¨í•œê±°
ì§ˆë¬¸ì : ì•Œì•˜ì–´ ë‹¤ ì°¨ë¦¬ë©´ ë¶€ë¥¼ê»˜
AI : ì–´ ~

"""
prompt_template = prompt.format(parents=parents, age=age, gender=gender, concept='ì¼ìƒ ëŒ€í™”') # Use .format() to insert values

# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±
prompt = PromptTemplate.from_template(prompt_template  + "\n\n#Question:\n{question}\n\n#Answer:")

# ì±—ë´‡ ìƒì„±
model = ChatOpenAI(model_name= "gpt-4o", temperature=0.7)

# ë©”ëª¨ë¦¬ ì„¤ì •
memory = ConversationSummaryBufferMemory(
    llm=model,
    max_token_limit=80,
    memory_key="chat_history"
)

def load_memory(input):
    return memory.load_memory_variables({})["chat_history"]

# ì²´ì¸ ìƒì„± í•¨ìˆ˜
def create_chain(prompt, model):
    chain = RunnablePassthrough.assign(chat_history=load_memory) | prompt | model | StrOutputParser()
    return chain

chain = create_chain(prompt, model)


st.text("-------------------------------------------------------------------------------"*3)
st.info(f'{age}ì„¸ {gender}ê³¼ ëŒ€í™”ë¥¼ ë‚˜ëˆ„ëŠ” ì±—ë´‡ì…ë‹ˆë‹¤ {parents} ì…ì¥ì—ì„œ ëŒ€í™”ë¥¼ ì‹œì‘í•´ë³´ì„¸ìš”')

# ì±„íŒ… í•¨ìˆ˜
def generate_response(user_input):
    # AI ì‘ë‹µ ìƒì„± (ìŠ¤íŠ¸ë¦¬ë° ì—†ìŒ)
    response = chain.invoke({"question": user_input})
    memory.save_context(
        {"input": user_input},
        {"output": response},
    )
    # print("ìë…€(20ì„¸ ì—¬ì„±) :", response)
    return response
   

if "messages" not in st.session_state:
    st.session_state["messages"] = [{"role": "user", "content": "ìë…€ì™€ì˜ ëŒ€í™”ë¥¼ ì‹œì‘í•´ë³´ì„¸ìš”"}]

for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])

if prompt := st.chat_input():
    st.session_state.messages.append({"role": "user", "content": prompt})
    st.chat_message("user").write(prompt)
    msg =  generate_response(prompt)
    st.session_state.messages.append({"role": "ai", "content": msg})
    st.chat_message("ai").write(msg)
